\documentclass[runningheads]{llncs}
%
\usepackage{amsmath, amssymb, lineno, lscape,hyperref,subcaption, verbatimbox, bm}
\usepackage[T1]{fontenc}
% T1 fonts will be used to generate the final print and online PDFs,
% so please use T1 fonts in your manuscript whenever possible.
% Other font encondings may result in incorrect characters.
%
\usepackage{graphicx}
% Used for displaying a sample figure. If possible, figure files should
% be included in EPS format.
%
% If you use the hyperref package, please uncomment the following two lines
% to display URLs in blue roman font according to Springer's eBook style:
%\usepackage{color}
%\renewcommand\UrlFont{\color{blue}\rmfamily}
%
\usepackage{color}
\newcommand{\todo}[1]{{\color{blue}TODO : {#1}}}

% Set control to 1 to include all proofs.
% Set control to 0 to shrink the paper to 12 pages
\def\control{1}

% Control of line numbering
%\linenumbers
% \renewcommand\linenumberfont{\normalfont\scriptsize}

% Graphics path
\graphicspath{{../fig/}}

\begin{document}
%
\title{The need for accuracy and smoothness in numerical simulations}
%
%\titlerunning{Abbreviated paper title}
% If the paper title is too long for the running head, you can set
% an abbreviated paper title here
%
\author{Carl Christian Kjelgaard Mikkelsen \inst{1}\orcidID{0000-0002-9158-1941 } \and
  Lori{\'e}n L{\'o}pez-Villellas \inst{2}\orcidID{0000-0002-1891-4359}
  % \and Pablo Garc{\'i}a-Risue{\~no} \inst{3} \orcidID{0000-0002-8142-9196}
}
%
\authorrunning{C. C. Kjelgaard Mikkelsen et al. }
% First names are abbreviated in the running head.
% If there are more than two authors, 'et al.' is used.
%
\institute{
  Department of Computing Science, Ume{\aa} University, 90187 Ume{\aa}, Sweden \email{spock@cs.umu.se} \\
  \and Barcelona Supercomputing Center, Barcelona, Spain, \\
  \email{lorien.lopez@bsc.es} \\
 %  \and Independent scholar \\
 %  \email{risueno@unizar.es, garcia.risueno@gmail.com}
}

%
\maketitle % typeset the header of the contribution
%
\begin{abstract} We consider the problem of estimating the error when solving a system of differential algebraic equations. Richardson extrapolation is a standard technique in numerical analysis that can be used to judge when rounding errors are irrelevant and estimate the discretization error. We have simulated molecular dynamics with constraints using GROMACS-library and found that the output is not always ameable to Richardson's analysis. We derive and illustrate Richardson's technique using a variety of numerical experiments. We identify two necessary conditions that are not always satisfied by GROMACS. 
\keywords{}
\end{abstract}
%
%
%

\section{Motivation} Consider the problem of solving the following system of differential algebraic equations
\begin{align}
  \bm{q}'(t) &= \bm{v}(t) \\
  \bm{M}\bm{v}'(t) &= \bm{f}(\bm{q}(t)) - \bm{G}(\bm{q}(t))^T\bm{\lambda}(t) \\
  \bm{g}(\bm{q}(t)) &= \bm{0}. 
\end{align}
This is Newton's law for a system of atoms moving in a force-field $\bm{f}$ subject to a set of constraints specified by the function $\bm{g}$.
The vector $\bm{q}$ represents the position of the atoms.
The vector $\bm{v}$ represents the velocities of the atoms.
The nonsingular diagonal matrix $\bm{M}$ lists the masses of the atoms.
The function $\bm{G}$ is the Jacobian of the function $\bm{g}$.
In the field of molecular dynamics, the standard algorithm for this problem is the SHAKE algorithm.
It uses a pair of staggered grids with uniform step size $h$ and takes the form
\begin{align}
  \bm{v}_{n+1/2} &= \bm{v}_{n-1/2} + \bm{h} \bm{M}^{-1} \left( \bm{f}(\bm{q}_n) - \bm{G}(\bm{q}_n)^T \bm{\lambda}_n \right), \\
  \bm{q}_{n+1} &= \bm{q}_n + h \bm{v}_{n + 1/2}, \\
  \bm{g}(\bm{q}_{n+1}) &= \bm{0}.
\end{align}
In reality, the constraint equation is a nonlinear equation with respect to the Lagrange multiplier $\bm{\lambda}_n$. 
Let $T$ denote any value that can be computed in terms of the trajectory $t \rightarrow (q(t), v(t))$ and let $A_h$ denote the corresponding value obtained from the output of the SHAKE algorithm.
Superficially, our task is to estimate $T - A_h$, but the presence of truncation and rounding errors ensure that we cannot hope to compute the exact value of $A_h$. 
Let $\hat{A}_h(\tau,u)$ denote the value obtained from running the SHAKE algorithm using floating point arithmetic with unit roundoff $u$ and solving the constraint equations with a relative error that is bounded by $\tau$.
This is the value returned by a computer. 
We have the following trivial identity
\begin{equation}
  T - \hat{A}_h(\tau,u) = T - A_h + A_h - \hat{A}_h(\tau,u)
\end{equation}
We see that the error $T-\hat{A}_h(\tau,u)$ can be written as sum of two terms.
The discretization error $E_h = T - A_h$ and the computational error $A_h - \hat{A}_h(\tau,u)$.

We can reduce the computational error by running the constraint solver until stagnation and by reducing the unit roundoff.
It is frequently possible to use Richardson extrapolation to determine when computational errors are insignificant and estimate the discretization error.
In the past, the authors have sought to accelerate the solution of nonlinear constraint equations in the context of molecular dynamics by integrating Newton's method into the GROMACS library for constrained molecular dynamics.
We have now discovered that the output of GROMACS is not always amenable to Richardson's analysis.
In this paper we derive and illustrate the use of Richardson's technique using a variety of numerical experiments.
We identify two conditions that are not always satified by GROMACS and we demonstrate that each condition is necessary for the succesful application of Richardson's techniques.
Our data and software is freely available from our GitHub along with every script needed to generate every number, table and figure from scratch.


\section{Theory}

Consider the problem of approximation a target value $T$ using a numerical method $A = A_h$ that depends on a single real parameter $h$. We shall assume there exists nonzero real constants $\alpha$ and $\beta$ and exponents
\begin{equation}
  0 < p < q < r
\end{equation}
such that the error $E_h = T - A_h$ satisfies
\begin{equation} \label{equ:aex}
  E_h  = \alpha h^p + \beta h^q + O(h^r), \quad h \rightarrow 0_+
\end{equation}
We say that the error $E_h$ satisfies an asymptotic error expansion. Frequently, the exponents $(p,q,r)$ are all integers, but since we shall allow exponents that are not integers, we must insist that $h$ is strictly positive.

Our first task to estimate the error $E_h$ for a specific value of $h$. Richardson's error estimate $R_h$ is defined by the equation
\begin{equation}
 R_h =  \frac{A_h - A_{2h}}{2^p - 1}
\end{equation}
The following theorem shows that Richardson's error estimate is a good approximation of the error when $h$ is sufficiently small.

\begin{theorem} If $E_h$ satisfies equation \eqref{equ:aex}, then
  \begin{equation}
    \frac{E_h - R_h}{h^q} \rightarrow  \left(1 - \frac{2^q-1}{2^p-1} \right) \beta, \quad h \rightarrow 0_+.
  \end{equation}
\end{theorem}
\begin{proof}
  By assumption, there is a function $g(h)$ 
  \begin{equation}
    T - A_h  = \alpha h^p + \beta h^q + g(h)
  \end{equation}
  and constants $C>0$ and $h_0 > 0$ such that
  \begin{equation}
    \forall h \leq h_0 \: : \: |g(h)| \leq Ch^r.
  \end{equation}
  It follows that
  \begin{equation}
    T- A_{2h} = 2^p \alpha h^p + 2^q \beta h^q + g(2h).
  \end{equation}
  We conclude that
  \begin{equation} \label{equ:Dh:1}
    A_h - A_{2h} = (2^p - 1) \alpha h^p + (2^q - 1) h^q + g(2h) - g(h).
  \end{equation}
  It follows that
  \begin{equation}
    R_h = \frac{A_h - A_{2h}}{2^p - 1} = \alpha h^p + \frac{2^q-1}{2^p-1} \beta h^q + \frac{g(2h)-g(h)}{2^p-1}.
  \end{equation}
 This implies that
  \begin{equation}
    \alpha h^p =  R_h - \frac{2^q-1}{2^p-1} \beta h^q + O(h^r).
  \end{equation}
  We conclude that
  \begin{equation}
    E_h = R_h + \left(1 - \frac{2^q-1}{2^p-1} \right) \beta h^q + O(h^r)
  \end{equation}
  The theorem follows immediately from this expression because $q < r$.
\end{proof}
We shall now demonstrate how to demonstrate the existence of an asymptotic error expansion experimentally.
We define Richardson's fraction $F_h$ using the expression
\begin{equation}
  F_h = \frac{A_{2h} - A_{4h}}{A_h - A_{2h}}.
\end{equation}
The behavior of the function $h \rightarrow F_h$ is described by the following theorem.
\begin{theorem} Assume that $T-A_h$ satisfies equation \eqref{equ:aex} and let $(m,n)$ be given by
  \begin{equation}
    m = q - p, \quad n = r - p.
  \end{equation}
  Then Richardson's fraction satisfies
  \begin{equation}
    F_h \rightarrow 2^p, \quad h \rightarrow 0_+
  \end{equation}
  and 
  \begin{equation}
    \frac{F_h - 2^p}{h^m} \rightarrow (2^m-1) \nu, \quad \nu = \frac{2^q-1}{2^p-1} \frac{\beta}{\alpha}.
    \end{equation}
\end{theorem}

\begin{proof} It is convenient to rewrite equation \eqref{equ:Dh:1} as
  \begin{equation}
    A_h - A_{2h} = (2^p-1) \alpha h^p \Big [ 1  + \nu h^m + \phi(h) \Big]
  \end{equation}
  where $\phi(h) \in O(h^n)$. It follows immediately that
   \begin{equation}
      A_{2h} - A_{4h} = 2^p (2^p-1) \alpha h^p \Big[ 1 + 2^m \nu h^m + \phi(2h) \Big]
  \end{equation}
  This allows us to write
  \begin{equation}
    F_h = \frac{ A_{2h} - A_{4h}}{ A_h - A_{2h}} = 2^p \left[ \frac{ 1 + 2^m \nu h^m + \phi(h)}{ 1  + \nu h^m + \phi(2h)} \right]
  \end{equation}
  The fraction on the right-hand side is of the form
  \begin{equation}
    \frac{1 + f(h)}{1 + g(h)} = 1 + \frac{f(h) - g(h)}{1 + g(h)}
  \end{equation}
  where
  \begin{equation}
    f(h) = 2^m \nu h^m + \phi(h), \quad g(h) = \nu h^m + \phi(h)
  \end{equation}
  It follows immediately that
  \begin{equation}
    F_h = 2^p \left( 1 + \frac{(2^m-1)\nu h^m}{1+g(h)} + \frac{\phi(h) - \phi(2h)}{1+g(h)}\right) \rightarrow 2^p, \quad h \rightarrow 0_+
  \end{equation}
  and
  \begin{equation}
    \frac{F_h - 2^p}{h^m}  = \frac{(2^m-1)\nu }{1+g(h)} + \frac{\phi(h) - \phi(2h)}{(1+g(h))h^m} \rightarrow (2^m - 1) \nu, \quad h \rightarrow 0_+,
  \end{equation}
  because $m < n$, so that
  \begin{equation}
     \frac{\phi(h) - \phi(2h)}{h^m} \rightarrow 0, \quad h \rightarrow 0_+.
  \end{equation}
  This completes the proof.
\end{proof}
We conclude that if the error $E_h$ satisfies equation \eqref{equ:aex}, then the order of the primary error term can be determined as the limit
\begin{equation}
p = \underset{h \rightarrow 0_+}{\lim} F_h
\end{equation}
and the difference $m = q-p$ can be determined from the fact
\begin{equation} \label{equ:asymtotic-behavior-of-Fh}
  \log |F_h - 2^p| \approx \log(2^m-1) + \log|\nu| + m \log(h)
\end{equation}
is a good approximation for $h$ sufficiently small. In particular, we note that the left-hand side of equation \ref{equ:asymtotic-behavior-of-Fh} is essentially a linear function of $\log(h)$ with slope $m$.
\section{Elementary examples}

The theory applies to the difference $E_h = T - A_h$between the target value $T$ and the exact value of the approximation $A_h$.
In practice, rounding errors ensure that the computed value $\hat{A}_h$ is different from the exact error $A_h$. However, it is frequently possible to assert that $A_h - \hat{A}_h$ is irrelevant and issue precise estimates of the computed error $\hat{E}_h = T - \hat{A}_h$. We shall now demonstrate the procedure. To this end, we consider the problem of computing definite integrals
\begin{equation} \label{equ:integral}
  T = \int_{a}^b f(x) dx
\end{equation}
using the composite trazoidal rule $A_h$ given by
\begin{equation}
  A_h = \frac{1}{2}h \sum_{j=0}^{n-1} \left[ f(x_j) + f(x_{j+1}) \right], \quad x_j = jh, \quad nh = b-a, \quad n \in \mathbb{N}.
\end{equation}
It is well-known that if the function $f : [a,b] \rightarrow \mathbb{R} $ is \emph{everywhere} smooth, then there exists a sequence $\{\alpha_j\}_{j=1}^\infty \subset \mathbb{R}$ such that 
\begin{equation}
  E_h = \sum_{j=1}^k \alpha _j h^{2j} + O(h^{2k+1}), \quad h \rightarrow 0_+
\end{equation}

\paragraph{Integration of a function that is everywhere smooth}

Let $[a,b]= [0,1]$, let $f : [a, b] \rightarrow \mathbb{R}$ be given by $f(x) = e^x$ and $T$ be given by equation \eqref{equ:integral}.
The script {\tt rint\_mwe1} computes the composite trapezoidal sum $A_h$ using $h_k = 2^{-k}$ for $k \in \{0,1,\dots,20\}$ and generates \ref{fig:rint_mwe1a} and \ref{fig:rint_mwe1b}.
The raw data shows that $\hat{A}_{h_k}$ approach $4 = 2^2$ as $k$ increases and $k \in \{2,3,\dots,14\}$.
This suggests that $p=2$.
Figure \ref{fig:rint_mwe1a} illustrates the evolution of the \emph{computed} values of Richardson's fraction.
We observe that $k \rightarrow \log_2|\hat{F}_{h_k} - 4|$ is essentially a linear function of $k$ with slope $-2$ for $k \in \{2,3,\dots,10\}$.
This is the so-called asymptotic range, where the computed value $\hat{A}_h$  behaves in a manner that is indistinguishable from the exact value $A_h$.
We conclude that the experiments supports the existence of an asymptotic error expansion with $(p,q) = (2,4)$.
Since the target value $T$ is known, we can treat Richardson's error estimate as an approximation of the error $T - \hat{A}_h$ and compute the corresponding relative error, see Figure \ref{fig:rint_mwe1b}.
We observe that computed value of Richardson's error estimate is a good approxmation of the error $T - \hat{A}_h$.

% tiny scriptsize footnotesize small normalsize large Large LARGE 

% \begin{myverbbox}[\scriptsize]{\valpha}
%  k |                A_h |         F_h |         R_h |         E_h | (E_h-R_h)/R_h
%  0 | 1.859140914230e+00 |  0.00000000 |  0.0000e+00 | -1.4086e-01 |    1.0000e+00
%  1 | 1.753931092465e+00 |  0.00000000 | -3.5070e-02 | -3.5649e-02 |    1.6251e-02
%  2 | 1.727221904558e+00 |  3.93908726 | -8.9031e-03 | -8.9401e-03 |    4.1402e-03
%  3 | 1.720518592164e+00 |  3.98447608 | -2.2344e-03 | -2.2368e-03 |    1.0400e-03
%  4 | 1.718841128580e+00 |  3.99610010 | -5.5915e-04 | -5.5930e-04 |    2.6031e-04
%  5 | 1.718421660316e+00 |  3.99902383 | -1.3982e-04 | -1.3983e-04 |    6.5098e-05
%  6 | 1.718316786850e+00 |  3.99975588 | -3.4958e-05 | -3.4958e-05 |    1.6276e-05
%  7 | 1.718290568083e+00 |  3.99993897 | -8.7396e-06 | -8.7396e-06 |    4.0690e-06
%  8 | 1.718284013367e+00 |  3.99998474 | -2.1849e-06 | -2.1849e-06 |    1.0167e-06
%  9 | 1.718282374686e+00 |  3.99999619 | -5.4623e-07 | -5.4623e-07 |    2.5759e-07
% 10 | 1.718281965016e+00 |  3.99999904 | -1.3656e-07 | -1.3656e-07 |    6.5583e-08
% 11 | 1.718281862598e+00 |  3.99999982 | -3.4139e-08 | -3.4139e-08 |    8.2385e-08
% 12 | 1.718281836994e+00 |  3.99999982 | -8.5348e-09 | -8.5348e-09 |    1.4743e-07
% 13 | 1.718281830593e+00 |  3.99999566 | -2.1337e-09 | -2.1337e-09 |   -3.7464e-06
% 14 | 1.718281828992e+00 |  3.99999681 | -5.3343e-10 | -5.3342e-10 |   -1.8177e-05
% 15 | 1.718281828592e+00 |  4.00004551 | -1.3335e-10 | -1.3335e-10 |   -2.7197e-05
% 16 | 1.718281828492e+00 |  4.00022202 | -3.3337e-11 | -3.3341e-11 |    1.1322e-04
% 17 | 1.718281828467e+00 |  4.00046186 | -8.3333e-12 | -8.3409e-12 |    9.1400e-04
% 18 | 1.718281828461e+00 |  3.98094194 | -2.0933e-12 | -2.0610e-12 |   -1.5658e-02
% 19 | 1.718281828460e+00 |  4.13782004 | -5.0589e-13 | -5.4334e-13 |    6.8928e-02
% 20 | 1.718281828459e+00 |  3.95086705 | -1.2805e-13 | -1.5921e-13 |    1.9572e-01
% \end{myverbbox}

% \begin{figure}
%   \centering
%   \valpha
%   \caption{Numerical integration of a function that is everywhere smooth.} \label{fig:rint_mwe1}
% \end{figure}


\begin{figure}[h]
\begin{subfigure}[h]{0.49\linewidth}
\includegraphics[width=\linewidth]{rint_mwe1a.png}
\caption{The evolution of $F_h$} \label{fig:rint_mwe1a}
\end{subfigure}
\hfill
\begin{subfigure}[h]{0.49\linewidth}
\includegraphics[width=\linewidth]{rint_mwe1b.png}
\caption{The size of $E_h$ and the accuracy of $R_h$} \label{fig:rint_mwe1b}
\end{subfigure}%
\caption{The evolution of $F_h$ and the accuracy of $R_h$ for a method with $(p,q) = (2,4)$.}
\end{figure}

\paragraph{Integration of a function that is smooth in all but one point}

Let $f : [0,1] \rightarrow \mathbb{R}$ be given by $f(x) = \sqrt{x}$ and let $T$ be given by equation \eqref{equ:integral}.
Then $T = \frac{2}{3}$. Since $f$ is not differentible at $x=0$ we have no guarantee that there exists an asymptotic error expansion.
The script {\tt rint\_mwe2} computes $A_h$ using $h_k = 2^{-k}$ for $k \in {0,1,\dots,25}$ and generates Figures \ref{fig:rint_mwe2a} and \ref{fig:rint_mwe2b}.
From the raw data it is clear $p = 2$ cannot be true, but it is plausible that $p = \frac{3}{2}$.
Figure \ref{fig:rint_mwe1a} illustrates the evolution of the \emph{computed} values of Richardson's fraction.
We observe that $k \rightarrow \log_2|\hat{F}_{h_k} - 2^{3/2}|$ is essentially a linear function of $k$ with slope $-\frac{1}{2}$ for $k \in \{2,3,\dots,18\}$.
This is the so-called asymptotic range where the computed numbers $\hat{A}_h$ behave in a manner that is indistinguishable from the exact value $A_h$.
We conclude that the experiment is consistent with an asymptotic error expansion with $(p,q)=(\tfrac{3}{2},2)$.
Since the target value $T$ is known, we can treat Richardson's error estimate as an approximation of the error $T-\hat{A}_h$ and compute the corresponding relative error, see Figure \ref{fig:rint_mwe2b}.
We observe that computed value of Richardson's error estimate is a good approxmation of the error $T - \hat{A}_h$.
In fact, the corresponding relative error decreases when $k$ increases and we remain inside the asymptotic region.

% \begin{myverbbox}[\scriptsize]{\vbeta}
%  k |                A_h |         F_h |         R_h |         E_h | (E_h-R_h)/R_h
%  0 | 5.000000000000e-01 |  0.00000000 |  0.0000e+00 |  1.6667e-01 |    1.0000e+00
%  1 | 6.035533905933e-01 |  0.00000000 |  5.6635e-02 |  6.3113e-02 |    1.0264e-01
%  2 | 6.432830462427e-01 |  2.60645075 |  2.1729e-02 |  2.3384e-02 |    7.0765e-02
%  3 | 6.581302216245e-01 |  2.67590667 |  8.1202e-03 |  8.5364e-03 |    4.8762e-02
%  4 | 6.635811968772e-01 |  2.72376496 |  2.9812e-03 |  3.0855e-03 |    3.3782e-02
%  5 | 6.655589362789e-01 |  2.75616456 |  1.0817e-03 |  1.1077e-03 |    2.3534e-02
%  6 | 6.662708113785e-01 |  2.77821124 |  3.8934e-04 |  3.9586e-04 |    1.6465e-02
%  7 | 6.665256572968e-01 |  2.79335492 |  1.3938e-04 |  1.4101e-04 |    1.1556e-02
%  8 | 6.666165489765e-01 |  2.80384210 |  4.9710e-05 |  5.0118e-05 |    8.1285e-03
%  9 | 6.666488815500e-01 |  2.81114895 |  1.7683e-05 |  1.7785e-05 |    5.7264e-03
% 10 | 6.666603622190e-01 |  2.81626213 |  6.2790e-06 |  6.3044e-06 |    4.0386e-03
% 11 | 6.666644335930e-01 |  2.81985125 |  2.2267e-06 |  2.2331e-06 |    2.8505e-03
% 12 | 6.666658761272e-01 |  2.82237604 |  7.8895e-07 |  7.9054e-07 |    2.0130e-03
% 13 | 6.666663869116e-01 |  2.82415482 |  2.7936e-07 |  2.7976e-07 |    1.4221e-03
% 14 | 6.666665676940e-01 |  2.82540939 |  9.8873e-08 |  9.8973e-08 |    1.0049e-03
% 15 | 6.666666316585e-01 |  2.82629477 |  3.4983e-08 |  3.5008e-08 |    7.1017e-04
% 16 | 6.666666542854e-01 |  2.82692054 |  1.2375e-08 |  1.2381e-08 |    5.0217e-04
% 17 | 6.666666622882e-01 |  2.82736352 |  4.3769e-09 |  4.3785e-09 |    3.5681e-04
% 18 | 6.666666651184e-01 |  2.82765569 |  1.5479e-09 |  1.5483e-09 |    2.3781e-04
% 19 | 6.666666661192e-01 |  2.82794845 |  5.4735e-10 |  5.4746e-10 |    1.9396e-04
% 20 | 6.666666664731e-01 |  2.82781700 |  1.9356e-10 |  1.9355e-10 |   -6.1533e-05
% 21 | 6.666666665982e-01 |  2.82856545 |  6.8431e-11 |  6.8428e-11 |   -3.5721e-05
% 22 | 6.666666666425e-01 |  2.82593073 |  2.4215e-11 |  2.4152e-11 |   -2.6041e-03
% 23 | 6.666666666581e-01 |  2.84485993 |  8.5119e-12 |  8.5889e-12 |    8.9627e-03
% 24 | 6.666666666636e-01 |  2.81554159 |  3.0232e-12 |  3.0612e-12 |    1.2421e-02
% 25 | 6.666666666657e-01 |  2.66194397 |  1.1357e-12 |  9.8466e-13 |   -1.5341e-01
% \end{myverbbox}

% \begin{figure}
%   \centering
%   \vbeta
%   \caption{Numerical integration of a function that is smooth in all but one point.}
%   \label{fig:rint_mwe2} 
% \end{figure}

\begin{figure}
\begin{subfigure}[h]{0.49\linewidth}
\includegraphics[width=\linewidth]{rint_mwe2a.png}
\caption{The evolution of $F_h$} \label{fig:rint_mwe2a}
\end{subfigure}
\hfill
\begin{subfigure}[h]{0.49\linewidth}
\includegraphics[width=\linewidth]{rint_mwe2b.png}
\caption{The size of $E_h$ and the accuracy of $R_h$.} \label{fig:rint_mwe2b}
\end{subfigure}%
\caption{The evolution $F_h$ and the accuracy of $R_h$ for a method with $(p,q) = (\frac{3}{2},2)$.}
\end{figure}

We mention in passing that low order methods are more pratical than high order methods in the sense that low order methods tend to have asymptotic ranges that are larger than high order methods.
This is due to the fact the function $h \rightarrow F_h$ suffers from subtractive cancellation when $h$ is sufficiently small and this issue is more acute for high order methods than for low order methods.


\section{Practical examples}

In this section we present the results of more elaborate examples that highlight the practical limitations of Richardson's extrapolation as well as some limitations of contemporary software for doing molecular dynamics. Every function or script necessary to replicate the numbers needed to reproduce the figures is freely available from the authors GitHub.

\subsection{A successful application of the theory}

In this section we shall demonstrate how the theory can be applied to analyze a nontrivial problem.

\paragraph{Example: Identify the shells fired by a howitzer}

Consider the D-20 howitzer whose effective range is known to be about 17.4 km. We have access to tables that describe the drag coefficient of 6 different shell types. Given the mass, diameter and muzzle velocity of the shells, can we determine the best possible match with any degree of confidence?

The script {\tt maxrange\_rk1} models a shells point particle moving in a plane subject to Earth's standard gravity and the international standard atmosperic model. Each trajectory is integrated using Euler's explicit method ({\tt 'rk1'}) and all but the final step has the same size $h$. The final step is adjusted to place the shell directly on the ground. While the relevant equation is linear and could be solved to machine precision, the bisection method is deliberately used to compute the final step with an error that is bounded by ${\tt tol} \cdot h$, where ${\tt tol}$ is set by the script. The drag function are interpolated from tables using cubic spline interpolation.

For each drag coefficient our target value $T$ is the maximum range of the shell as the elevation varies continuously from $0$ to $\frac{\pi}{2}$. For each drag coefficient, we compute $12$ different approximations $A_{h_k}$ of $T$ using the step size $h_k = 2^{3-k}$ seconds, where $k \in \{1,2,\dots,12\}$.
For each drag coefficient and for each value of the time step $h$, a range function is defined which returns the range of the shell as a function of the howitzers elevation $\theta$.
The range functions are unimodal and the maximum range is found using the golden section search algorithm.
The initial search bracket is $[0,\pi/2$] and this bracket is systematically reduced in length until it is shorter than $\frac{pi}{2}u$.

The script will either read the raw data from a file or generate it from scratch. In any case, the script produces several figures and tables including Figure \ref{fig:maxrange_rk1_table_tol53} and Figure \ref{fig:maxrange_rk1_fraction_tol53}. These figures represent calculations where the shell is placed on the ground with ``excessive'' accuracy in the sense that the final step is computed with an error that is bounded $u h_{k}$ where $u = 2^{-53}$ is the double precision unit roundoff. Figure \ref{fig:maxrange_rk1_table_tol53} list the maximum range and the corresponding error estimate for each of the shells know to us computed using a time step of $h = 2^{-9} s$. In each case the error estimate suggests that the computed range is exact to number of figures shown. In particular, we see while a G7 type shell achieves a maximum range of 17.4 km, all other shells have ranges that are less than 16.9 km. However, it is a fallacy to conclude anything on the basis of the computed ranges and the associated error estimates. We need to assert that we are inside the asymptotic range and that the error estimates are reliable. To this end, we examine the evolution of Richardson's fraction for the maximum range of each shell, see Figure \ref{fig:maxrange_rk1_fraction_tol53}. For each of the 6 different drag coefficients we find that evolution of Richardson's fraction supports an asymptotic error expansion with $(p,q) = (1,2)$. This result is entirely consistent with the use of Euler's explicit method. We observe that for each drag coefficient, $k=12$ is still inside the asymtotic range and we have no reason to doubt the quality of the error estimate. We conclude that the best shell model for the D-20 howitzer is the G7 shell.

\begin{myverbbox}[\normalsize]{\vgamma}
Shell type  Maximum range (m) Error estimate (m)
    G1           12832                0.4 
    G2           16857                0.1 
    G5           15918                0.2 
    G6           15556                0.2 
    G7           17461                0.1 
    G8           15914                0.1 
\end{myverbbox}

\begin{figure}
  \centering
  \vgamma
  \caption{The computed maximum range for 6 different shell types fired from the D-20 howitzer.}
  \label{fig:maxrange_rk1_table_tol53} 
\end{figure}

\begin{figure}
  \centering
  \includegraphics[width=12cm]{maxrange_rk1_tol53.png}
  \caption{The evolution of Richardson's fraction corresponding to the maximum range of 6 different shells fired from the D-20 howitzer.} \label{fig:maxrange_rk1_fraction_tol53}
\end{figure}



\subsection{An unsuccesful application of the theory}

In this section we present the experiment that triggered the production of the present paper. We attempted to apply Richardson's techniques in the context of molecular dynamics with constraints. It was a spectacular failure with no obvious pattern controlling the computed numbers. While we cannot claim to fully understand our failure, we have identified two key differences between the our two test cases and we have through a sequence of experiments verified that each difference is enough to destroy any hope of having an asymptotic error expansion.

We utilized GROMACS v2021 to conduct experiments on the behavior of hen egg white lysozyme submerged in water within a cubic simulation box, following Justin Lemkul's Lysozyme in Water GROMACS Tutorial. Several steps were taken to prepare the system for production simulation: first, ions were introduced to achieve electrical neutrality. Subsequently, energy minimization was performed using the steepest descent algorithm until the maximum force reached below 1000.0 kJ/(mol·nm). Following this, the system underwent 100 ps of equilibration in an NVT ensemble to stabilize temperature, followed by another 100 ps of equilibration in an NPT ensemble to stabilize pressure. The described process was replicated using two different force fields, OPLS-AA/L and CHARMM36. We conducted production simulations of 1 ps for both force fields, varying the number of executed steps from 250 to 16000 and employing two different tolerances for the SHAKE algorithm (1E-4 and 1E-12). For each experiment we computed the total kinetic and potential energy of the system at the end of the simulation.

\begin{figure}
  \centering
  \includegraphics[width=12cm]{charmm36tol04.pdf}
\end{figure}

\section{The need for sufficient accuracy}






There are reasons beyond error estimation for solving the constraint equations accurately. The chemistry literature contains several papers that bemoan the fact that the energy drift is unacceptably large unless the constraint equations are solved with sufficient accuracy. However, what is perhaps less well known is \emph{direction} of the energy drift can depend on the choice of algorithm used to solve the constraint equations! We shall now present two examples that illustrate this point. We shall simulate the motion of a simple pendulum over a large number of periods. The only difference between the two simulation shall be the choice of algorithm used solve the scalar constraint equation.

\paragraph{Example: A pendulum that gains energy through numerical error}

\paragraph{Example: A pendulum that loses energy through numerical error} 

\section{The need for sufficient smoothness}




\section{Conclusion}



\subsubsection{Acknowledgments}

The first author is supported by eSSENCE, a collaborative e-Science programme funded by the Swedish Research Council within the framework of the strategic research areas designated by the Swedish Government.

% @Lorien: update these lines with current information.
% This work has been partially supported by the Spanish Ministry of Science and Innovation (contract PID2019-107255GB-C21/AEI/10.13039/501100011033), by the Generalitat de Catalunya (contract 2017-SGR-1328), and by Lenovo-BSC Contract-Framework Contract (2020).

%
% ---- Bibliography ----
%
% BibTeX users should specify bibliography style 'splncs04'.
% References will then be sorted and formatted in the correct style.
%
\bibliographystyle{splncs04}
\bibliography{references}
 
\end{document}

